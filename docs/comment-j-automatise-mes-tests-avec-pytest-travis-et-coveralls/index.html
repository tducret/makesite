<!DOCTYPE html>
<link rel="icon" href="data:;base64,=">
<meta charset="utf-8">
<title>Comment j'automatise mes tests avec Pytest, Travis et Coveralls - tducret.com</title>
<link rel="stylesheet" href="/makesite/style.css">

<header>
    <h1>[td]</h1>
    <nav><a href="/makesite/">Accueil</a> | <a href="https://github.com/tducret" target="_blank">Github</a> | <a href="https://www.linkedin.com/in/thibaultducret" target="_blank">Linkedin</a> | <a href="https://visualping.io/?url=tducret.com&mode=text" target="_blank" title="Soyez notifiÃ© des nouveaux articles">ğŸ””</a></nav>
</header>

<h1>Comment j'automatise mes tests avec Pytest, Travis et Coveralls</h1>
<p>Lorsque l'on crÃ©e un projet en Python, on peut avoir tendance Ã  se focaliser sur le dÃ©veloppement des fonctionnalitÃ©s.</p>
<ul>
<li>J'ai un objectif,</li>
<li>je rÃ©flÃ©chis aux fonctions Ã  Ã©crire,</li>
<li>je les code,</li>
<li>je les teste,</li>
<li>je modifie ce qui pose problÃ¨me,</li>
<li>et je continue jusqu'Ã  avoir le comportement souhaitÃ©.</li>
</ul>
<p>Le problÃ¨me avec cette approche, c'est qu'en reprenant le code quelques semaines ou mois plus tard, on ne se souvient plus de certains dÃ©tails du contenu. On continue, on fait des Ã©volutions... et <strong>CRAC</strong>. Ã‡a ne fonctionne plus comme avant ğŸ˜­</p>
<p><strong>D'oÃ¹ vient le problÃ¨me ?</strong></p>
<p>Il va falloir analyser pas-Ã -pas le dÃ©roulement du programme, fonction aprÃ¨s fonction, paramÃ¨tre d'entrÃ©e aprÃ¨s paramÃ¨tre d'entrÃ©e, ligne aprÃ¨s ligne...</p>
<p>Au mieux, avec un debugger (<a href="https://docs.python.org/3/library/pdb.html">pdb</a> en Python). Au pire, en faisant des <strong>print</strong> Ã  des endroits stratÃ©giques (je le sais, je fais pareil !).</p>
<p>La tentation est alors grande de jeter l'Ã©ponge si le problÃ¨me est vraiment complexe.</p>
<p>Je vais vous expliquer dans cet article comment amÃ©liorer la maintenabilitÃ© de votre code. Ou dit diffÃ©remment, vous faciliter la vie !</p>
<p>Vous apprendrez comment ajouter des tests Ã  votre projet, qui seront jouÃ©s automatiquement lors d'une modification de votre code. Vous pourrez ainsi dÃ©tecter la moindre rÃ©gression, au plus vite.
En fin d'article, vous pourrez mÃªme vÃ©rifier le taux de couverture de vos tests. C'est-Ã -dire le pourcentage de lignes de code par lesquels passent vos tests.</p>
<h1>Pytest</h1>
<p>Tester son programme revient Ã  Ã©crire des appels aux fonctions de votre programme, et indiquer la valeur attendue en sortie.</p>
<p>Un outil est alors utilisÃ© pour lire les tests, les exÃ©cuter, et vous prÃ©venir si le comportement attendu n'est pas le bon.</p>
<p>Supposons qu'on ait Ã©crit une fonction pour faire la somme de deux nombres :</p>
<pre><code class="language-python">def addition(a, b):
    return a+b
</code></pre>
<p>Le test pourrait Ãªtre le suivant :</p>
<pre><code class="language-python">def test_addition_2_plus_3():
    assert addition(2,3) == 5
</code></pre>
<p><code>assert</code> est une notion qui revient souvent dans les tests logiciels. En franÃ§ais, on peut le traduire par <em>affirmer que</em>. On utilise <code>assert</code> en indiquant derriÃ¨re le test Ã  vÃ©rifier. Il doit Ãªtre vrai pour que le teste passe.</p>
<pre><code class="language-python">def test_addition_4_plus_5():
    assert addition(4,5) == 10
    # Ce test va Ã©chouer !
</code></pre>
<p>Il existe plusieurs outils pour exÃ©cuter les tests en Python.</p>
<p>Voici mon <strong>TOP 3</strong> :</p>
<ul>
<li><a href="https://docs.python.org/3/library/unittest.html">unittest</a> : l'outil intÃ©grÃ© Ã  Python</li>
<li><a href="http://nose.readthedocs.io/en/latest/">nose</a> : l'outil qui Ã©tend les fonctions de unittest</li>
<li><a href="https://docs.pytest.org/en/latest/">pytest</a> : du mÃªme type que nose, mais avec une syntaxe diffÃ©rente</li>
</ul>
<p>Lors de mes recherches, <strong>pytest</strong> a retenu mon attention par sa syntaxe simple et sa compatibilitÃ© avec de nombreux outils externes (cf. Travis, que je vous prÃ©sente un peu plus bas).</p>
<p><img src="/assets/article_images/2018-06-24-comment-j-automatise-mes-tests-avec-pytest-travis-et-coveralls/livre_pytest.jpg" alt="Python Testing with pytest" /></p>
<p>Un trÃ¨s bon livre lui est d'ailleurs dÃ©diÃ© : <a href="http://pythontesting.net/books/pytest/">Python Testing with pytest</a>.</p>
<p><strong>nose</strong> est vraiment comparable, et il revient Ã  chacun de se faire sa propre idÃ©e.</p>
<h2>Mise en place des tests</h2>
<p>Par convention, on crÃ©e Ã  la racine du projet un dossier <code>test</code>, dans lequel on crÃ©e des fichiers de tests du type <code>test_*.py</code> (un fichier par thÃ¨me ou module). Cela permettra Ã  <strong>pytest</strong> de les trouver tout seul.</p>
<p>Voici Ã  quoi ressemble la structure de mon projet <a href="https://github.com/tducret/amazon-scraper-python/">amazon-scraper-python</a> :</p>
<pre><code class="language-bash">.
â”œâ”€â”€ .travis.yml
â”œâ”€â”€ Dockerfile
â”œâ”€â”€ LICENSE
â”œâ”€â”€ MANIFEST.in
â”œâ”€â”€ README.md
â”œâ”€â”€ amazon2csv.py
â”œâ”€â”€ amazonscraper
â”‚Â Â  â”œâ”€â”€ __init__.py
â”‚Â Â  â”œâ”€â”€ client.py
â”œâ”€â”€ pytest.ini
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ setup.cfg
â”œâ”€â”€ setup.py
â””â”€â”€ test
    â””â”€â”€ test_amazonscraper.py
</code></pre>
<p>On voit justement <code>test/test_amazonscraper.py</code>.</p>
<p>Jeton un oeil Ã  l'intÃ©rieur de <strong>test_amazonscraper.py</strong> :</p>
<pre><code class="language-python">import amazonscraper

def test_amazonscraper_get_products_with_keywords():

    products = amazonscraper.search(
                                keywords=&quot;Python&quot;,
                                max_product_nb=10)

    assert len(products) == 10
</code></pre>
<p>Ce test permet de vÃ©rifier que l'appel Ã  la fonction de recherche (avec le mot-clÃ© &quot;Python&quot; et un nombre de rÃ©sultats maximal de 10) nous renvoie bien 10 rÃ©sultats.</p>
<p>Pour exÃ©cuter le test, on s'assure tout d'abord d'avoir installÃ© <strong>pytest</strong> :</p>
<pre><code class="language-bash">pip install -U pytest
</code></pre>
<p>On se place ensuite dans le rÃ©pertoire du projet, et on lance la commande <code>pytest -v</code> (<em>v</em> est l'option <em>verbose</em> pour avoir plus de dÃ©tails).</p>
<p><img src="/assets/article_images/2018-06-24-comment-j-automatise-mes-tests-avec-pytest-travis-et-coveralls/pytest_test_amazonscraper_get_products_with_keywords.png" alt="RÃ©sultat de la commande pytest" /></p>
<h2>Mock</h2>
<p><em>(ajoutÃ© le 26/11/2018)</em></p>
<p><a href="https://docs.python.org/dev/library/unittest.mock.html"><code>Mock</code></a> est un moyen trÃ¨s pratique de tester des fonctions nÃ©cessitant des appels Ã  d'autres librairies ou API. Celui-ci <strong>imite</strong> cette fonction en la remplaÃ§ant par une fonction de votre choix.</p>
<p>Voici un exemple (rÃ©cupÃ©rÃ© <a href="https://rhodesmill.org/brandon/slides/2014-07-pyohio/clean-architecture/">ici</a>) qui remplace la requÃªte vers un serveur web (ici une recherche de dÃ©finition via l'API de DuckDuckGo) par une fonction interne qui renvoie une dÃ©finition de notre choix.</p>
<p>La fonction originale Ã  tester :</p>
<pre><code class="language-python">import requests

def find_definition(word):
    q = 'define ' + word
    url = 'http://api.duckduckgo.com/?'
    url += urlencode({'q': q, 'format': 'json'})
    response = requests.get(url)     # &lt;== On va imiter cette fonction
    data = response.json()           # &lt;== ainsi que celle-ci
    definition = data[u'Definition']
    if definition == u'':
        raise ValueError('that is not a word')
    return definition
</code></pre>
<p>Le test :</p>
<pre><code class="language-python">from mock import patch

class FakeRequestsLibrary(object):
    def get(self, url):
        self.url = url
        return self
    def json(self):
        return self.data

def test_find_definition():
    fake = FakeRequestsLibrary()
    fake.data = {u'Definition': u'abc'}
    with patch('requests.get', fake.get):  # &lt;== On la remplace ici
        definition = find_definition('testword')

    assert definition == 'abc'
    assert fake.url == (
        'http://api.duckduckgo.com/'
        '?q=define+testword&amp;format=json')
</code></pre>
<blockquote>
<p>Cette possibilitÃ© de test sera particuliÃ¨rement utile quand vous souhaiterez tester la logique de votre application sans dÃ©pendre d'un appel Ã  une API externe (qui pourrait aussi allonger la durÃ©e du test).</p>
</blockquote>
<h2>Doctest</h2>
<p>En plus des tests dans un fichier dÃ©diÃ© (du type <code>test_*.py</code>), on peut Ã©crire des tests directement dans le code.
On les appelle les <strong>doctest</strong>.</p>
<p>Dans le commentaire de description de la fonction (appelÃ© Ã©galement <em>docstring</em>), il suffit de dÃ©crire l'appel Ã  tester (prÃ©cÃ©dÃ© de <code>&gt;&gt;&gt;</code>), et le retour attendu.</p>
<p>Reprenons l'exemple de tout Ã  l'heure, oÃ¹ l'on souhaite tester que la fonction <strong>addition</strong> est bien capable de faire <code>2+3 = 5</code> :</p>
<pre><code class="language-python">def addition(a, b):
    &quot;&quot;&quot; Fonction qui additionne 2 nombres
    &gt;&gt;&gt; addition(2,3)
    5
    &quot;&quot;&quot;
    return a+b
</code></pre>
<p>C'est trÃ¨s pratique, car en plus de donner un exemple d'utilisation pour documenter le code, on vient de crÃ©er un test automatisÃ©.</p>
<p>Il suffit alors d'appeler <code>pytest --doctest-modules</code> pour que ces doctests soient Ã©galement jouÃ©s.</p>
<p>On peut Ã©galement dÃ©crire des cas d'erreurs, par exemple :</p>
<pre><code class="language-python">def addition(a, b):
    &quot;&quot;&quot; Fonction qui additionne 2 nombres
    &gt;&gt;&gt; addition(2,3)
    5
    &gt;&gt;&gt; addition(&quot;a&quot;,5)
    Traceback (most recent call last):
    ...
    TypeError: must be str, not int
    &quot;&quot;&quot;
    return a+b
</code></pre>
<p><em>(les <code>...</code> entre <strong>Traceback</strong> et <strong>TypeError</strong> permettent de ne pas recopier intÃ©gralement l'erreur.)</em></p>
<p><img src="/assets/article_images/2018-06-24-comment-j-automatise-mes-tests-avec-pytest-travis-et-coveralls/pytest_doctest_addition_OK.png" alt="RÃ©sultat de la commande pytest avec doctest : OK" /></p>
<p>Pour vous montrer ce qui se passerait en cas d'Ã©chec d'un test (suite Ã  une rÃ©gression par exemple), je modifie la valeur attendue pour 2+3, en mettant 6.</p>
<p>Voici le retour, qui indique clairement l'endroit oÃ¹ le test Ã©choue.</p>
<p><img src="/assets/article_images/2018-06-24-comment-j-automatise-mes-tests-avec-pytest-travis-et-coveralls/pytest_doctest_addition_NOK.png" alt="RÃ©sultat de la commande pytest avec doctest : NOK" /></p>
<h2>Travis</h2>
<p>Nous avons dÃ©sormais de bonnes bases pour que le code soit plus facile Ã  maintenir. Il suffit d'exÃ©cuter rÃ©guliÃ¨rement la commande <strong>pytest</strong> pour s'assurer que tout fonctionne correctement...</p>
<p>Et si on pouvait Ã©galement automatiser cette Ã©tape ğŸ¤”</p>
<p><img src="/assets/article_images/2018-06-24-comment-j-automatise-mes-tests-avec-pytest-travis-et-coveralls/travis.png" alt="Travis CI" /></p>
<p>C'est ici qu'entre en jeu <a href="https://travis-ci.org/">Travis CI</a>.</p>
<p>Le principe de tester rÃ©guliÃ¨rement le code n'est pas nouveau : on parle d'intÃ©gration continue.</p>
<p>NÃ©anmoins, <strong>Travis</strong> prÃ©sente l'avantage d'Ãªtre trÃ¨s simple d'utilisation et gratuit pour les projets open source.</p>
<p>Dans votre projet, vous rajoutez un fichier <code>.travis.yml</code>, avec les consignes d'installation et de lancement des tests.
Voici Ã  quoi Ã§a ressemble pour un de mes projets :</p>
<pre><code class="language-yml">language: python
python:
  - &quot;3.6&quot;
# Commande pour installer votre code
install:
  - pip install .
# Commandes pour installer les dÃ©pendances
before_script:
  - pip install -r requirements.txt
# Commande pour exÃ©cuter les tests
script:
  - pytest
</code></pre>
<p>Rendez-vous ensuite sur <a href="https://travis-ci.org/">le site de Travis-CI</a> pour vous inscrire (via les identifiants de votre compte Github).</p>
<p>Vous pourrez alors choisir les repository des projets Github Ã  tester.</p>
<p><img src="/assets/article_images/2018-06-24-comment-j-automatise-mes-tests-avec-pytest-travis-et-coveralls/travis_selection_repository.png" alt="SÃ©lection des projets Ã  tester sur Travis" /></p>
<p>Il suffit alors de faire un commit pour dÃ©clencher les tests sur Travis.</p>
<p><img src="/assets/article_images/2018-06-24-comment-j-automatise-mes-tests-avec-pytest-travis-et-coveralls/build_travis.png" alt="Visualisation du dÃ©roulement des tests sur travis-ci.org" /></p>
<p>Et si jamais cela ne se passait pas comme prÃ©vu, vous recevez un petit mail :</p>
<p><img src="/assets/article_images/2018-06-24-comment-j-automatise-mes-tests-avec-pytest-travis-et-coveralls/mail_travis_build_broken.png" alt="Exemple de mail envoyÃ© par Travis quand une modification de code provoque une erreur" /></p>
<p>Ce n'est qu'un aperÃ§u, les possibilitÃ©s sont trÃ¨s nombreuses :</p>
<ul>
<li>plusieurs types de serveurs de tests</li>
<li>plusieurs versions de Python</li>
<li>dÃ©pÃ´t du package gÃ©nÃ©rÃ© sur Pypi</li>
<li>dÃ©ploiement automatique</li>
<li>envoi de notifications (email, IRC, Slack, webhooks...)</li>
</ul>
<p>Plus d'infos sur <a href="https://docs.travis-ci.com/user/getting-started/">la documentation Travis</a></p>
<h2>Coveralls</h2>
<p>Un dernier point Ã  regarder de prÃ¨s quand on fait des tests : la couverture de code.
Il s'agit d'analyser par quelles lignes passent vos tests... et par quelles lignes ils ne passent pas.
On fait alors un rapport entre le nombre de lignes <em>couvertes</em> et le nombre de lignes total.
On obtient un pourcentage : le taux de couverture de code, qui donne une indication sur la qualitÃ© des tests sur le projet.
Un projet avec 50% de taux de couverture de code donne une mauvaise image, car les tests mis en place passent seulement par la moitiÃ© des lignes de code. On risque de garder de nombreux morceaux de code obsolÃ¨tes, sans le savoir, jusqu'au jour oÃ¹...</p>
<p>On peut faire cette analyse trÃ¨s facilement avec <strong>pytest</strong> et le plugin <strong>pytest-cov</strong>.</p>
<pre><code class="language-bash">pip install pytest-cov
</code></pre>
<p>Exemple d'utilisation :</p>
<pre><code class="language-bash">pytest --cov=myproj tests/

-------------------- coverage: ... ---------------------
Name                 Stmts   Miss  Cover
----------------------------------------
myproj/__init__          2      0   100%
myproj/myproj          257     13    94%
myproj/feature4286      94      7    92%
----------------------------------------
TOTAL                  353     20    94%
</code></pre>
<p>Pas mal du tout, mais on peut faire mieux !</p>
<p><img src="/assets/article_images/2018-06-24-comment-j-automatise-mes-tests-avec-pytest-travis-et-coveralls/coveralls.png" alt="Coveralls.io" /></p>
<p>Un service existe sur le web pour faciliter cette analyse : <a href="http://coveralls.io/">Coveralls</a></p>
<p>C'est Ã©galement gratuit pour les projets open source, et on peut s'inscrire avec son compte Github.</p>
<p>En connectant Coveralls Ã  Travis, on peut gÃ©nÃ©rer l'analyse de couverture de code automatiquement.</p>
<p>Pour cela, on ajoute simplement quelques lignes de plus dans <code>.travis.yml</code></p>
<pre><code class="language-yml">language: python
python:
  - &quot;3.6&quot;
# Commande pour installer votre code
install:
  - pip install .
# Commandes pour installer les dÃ©pendances
before_script:
  - pip install -r requirements.txt
  - pip install python-coveralls
  - pip install pytest-cov
# Commande pour exÃ©cuter les tests
script:
  - pytest
# Commande pour envoyer les rÃ©sultats de couverture de code Ã  coveralls.io
after_success:
  coveralls
</code></pre>
<p>J'ai Ã©galement ajoutÃ© un fichier <code>pytest.ini</code> au mÃªme endroit, pour que pytest puisse rÃ©cupÃ©rer ses paramÃ¨tres d'appel :</p>
<pre><code>[pytest]
addopts = --doctest-modules --cov amazonscraper
</code></pre>
<p>AprÃ¨s chaque commit sur Github, Travis va installer le code, exÃ©cuter tous mes tests, et envoyer la couverture de code Ã  Coveralls. Je peux alors connaitre mon taux de couverture sur Coveralls, et analyser les lignes non couvertes :</p>
<p><img src="/assets/article_images/2018-06-24-comment-j-automatise-mes-tests-avec-pytest-travis-et-coveralls/coveralls_lignes_non_couvertes.png" alt="Lignes non couvertes sur Coveralls" /></p>
<p>Dans mon cas, il s'agit d'un cas bien particulier : la gestion d'une exception en cas d'Ã©chec de communication SSL avec les serveurs d'Amazon. C'est donc difficile Ã  tester de maniÃ¨re automatique (car Ã§a n'est pas systÃ©matique).
On peut nÃ©anmoins simuler ce comportement pour les tests Ã  l'aide de <a href="#mock"><strong>Mock</strong></a>.</p>
<p><img src="/assets/article_images/2018-06-24-comment-j-automatise-mes-tests-avec-pytest-travis-et-coveralls/badges_github.png" alt="Badges sur Github" /></p>
<p><strong>BONUS</strong> : Vous pouvez alors afficher fiÃ¨rement l'Ã©tat des tests et la couverture de votre code, sur la page Github de votre projet. Les badges seront mis Ã  jour dynamiquement Ã  chaque modification de votre programme.</p>
<p><img src="https://media.giphy.com/media/vq4q4LqJv3Qcg/giphy.gif" alt="Test successful" /></p>
<p>Si vous ne voulez manquer aucun article, soyez notifiÃ© directement dans votre boite mail <a href="http://bit.ly/newsletter-tducret">en vous inscrivant Ã  la newsletter</a></p>

<footer></footer>
</html>